{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aY9STWi2f1K9",
    "outputId": "38a9d9ab-8975-4b4a-856c-333baf7cc651"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gULhX_mpXpkF",
    "outputId": "d4639664-8614-4b55-d3af-cf78f68242fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wget in /usr/local/lib/python3.6/dist-packages (3.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mxcetbMWf2Ga",
    "outputId": "d8399ed8-2f89-47c4-bb8b-d0859dca4dd0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/MyDrive/M2/DeepLearning\n"
     ]
    }
   ],
   "source": [
    "%cd /content/drive/MyDrive/M2/DeepLearning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zsCTG8HUcpdI",
    "outputId": "9cc61883-06c5-4ad7-db14-f2c8c24ff5be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/MyDrive/M2/DeepLearning/deeplearningproject\n"
     ]
    }
   ],
   "source": [
    "cd deeplearningproject/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yfV2WAKJX8Bp",
    "outputId": "472e5f70-8a2d-4071-a1aa-d8d82a429a68"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow 1.x selected.\n"
     ]
    }
   ],
   "source": [
    "%tensorflow_version 1.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "sU2-hBAFXhjZ"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "from utils import get_init_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zKSTuw63Xhja"
   },
   "source": [
    "## ENV Prepa'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "30mF5YvKXhjb"
   },
   "source": [
    "### Download Glove if doesn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cIn3oYBiXhjc",
    "outputId": "93df882c-208c-4841-9943-7f957a2a7bfa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import wget\n",
    "from os import path\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "glove_dir = \"glove\"\n",
    "glove_url = \"https://nlp.stanford.edu/data/wordvecs/glove.42B.300d.zip\"\n",
    "\n",
    "\n",
    "# Download glove vector if not exit\n",
    "if not path.exists(\"glove\"):\n",
    "    if not os.path.exists(glove_dir):\n",
    "        os.mkdir(glove_dir)\n",
    "    wget.download(glove_url, out=glove_dir)\n",
    "    # Extract glove file\n",
    "    with zipfile.ZipFile(os.path.join(\"glove\", \"glove.6B.300d.zip\"), \"r\") as z:\n",
    "        z.extractall(glove_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FFk7LkooXhjc"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "0RhXBQXuXhjd"
   },
   "outputs": [],
   "source": [
    "class Model(object):\n",
    "    def __init__(self, reversed_dict, article_max_len, summary_max_len, args, forward_only=False):\n",
    "        self.vocabulary_size = len(reversed_dict)\n",
    "        self.embedding_size = args.embedding_size\n",
    "        self.num_hidden = args.num_hidden\n",
    "        self.num_layers = args.num_layers\n",
    "        self.learning_rate = args.learning_rate\n",
    "        self.beam_width = args.beam_width\n",
    "        if not forward_only:\n",
    "            self.keep_prob = args.keep_prob\n",
    "        else:\n",
    "            self.keep_prob = 1.0\n",
    "        self.cell = tf.nn.rnn_cell.BasicLSTMCell\n",
    "        with tf.variable_scope(\"decoder/projection\"):\n",
    "            self.projection_layer = tf.layers.Dense(self.vocabulary_size, use_bias=False)\n",
    "\n",
    "        self.batch_size = tf.placeholder(tf.int32, (), name=\"batch_size\")\n",
    "        self.X = tf.placeholder(tf.int32, [None, article_max_len])\n",
    "        self.X_len = tf.placeholder(tf.int32, [None])\n",
    "        self.decoder_input = tf.placeholder(tf.int32, [None, summary_max_len])\n",
    "        self.decoder_len = tf.placeholder(tf.int32, [None])\n",
    "        self.decoder_target = tf.placeholder(tf.int32, [None, summary_max_len])\n",
    "        self.global_step = tf.Variable(0, trainable=False)\n",
    "\n",
    "        with tf.name_scope(\"embedding\"):\n",
    "            if not forward_only and args.glove:\n",
    "                init_embeddings = tf.constant(get_init_embedding(reversed_dict, self.embedding_size), dtype=tf.float32)\n",
    "            else:\n",
    "                init_embeddings = tf.random_uniform([self.vocabulary_size, self.embedding_size], -1.0, 1.0)\n",
    "            self.embeddings = tf.get_variable(\"embeddings\", initializer=init_embeddings)\n",
    "            self.encoder_emb_inp = tf.transpose(tf.nn.embedding_lookup(self.embeddings, self.X), perm=[1, 0, 2])\n",
    "            self.decoder_emb_inp = tf.transpose(tf.nn.embedding_lookup(self.embeddings, self.decoder_input), perm=[1, 0, 2])\n",
    "\n",
    "        with tf.name_scope(\"encoder\"):\n",
    "            fw_cells = [self.cell(self.num_hidden) for _ in range(self.num_layers)]\n",
    "            bw_cells = [self.cell(self.num_hidden) for _ in range(self.num_layers)]\n",
    "            fw_cells = [rnn.DropoutWrapper(cell) for cell in fw_cells]\n",
    "            bw_cells = [rnn.DropoutWrapper(cell) for cell in bw_cells]\n",
    "\n",
    "            encoder_outputs, encoder_state_fw, encoder_state_bw = tf.contrib.rnn.stack_bidirectional_dynamic_rnn(\n",
    "                fw_cells, bw_cells, self.encoder_emb_inp,\n",
    "                sequence_length=self.X_len, time_major=True, dtype=tf.float32)\n",
    "            self.encoder_output = tf.concat(encoder_outputs, 2)\n",
    "            encoder_state_c = tf.concat((encoder_state_fw[0].c, encoder_state_bw[0].c), 1)\n",
    "            encoder_state_h = tf.concat((encoder_state_fw[0].h, encoder_state_bw[0].h), 1)\n",
    "            self.encoder_state = rnn.LSTMStateTuple(c=encoder_state_c, h=encoder_state_h)\n",
    "\n",
    "        with tf.name_scope(\"decoder\"), tf.variable_scope(\"decoder\") as decoder_scope:\n",
    "            decoder_cell = self.cell(self.num_hidden * 2)\n",
    "\n",
    "            if not forward_only:\n",
    "                attention_states = tf.transpose(self.encoder_output, [1, 0, 2])\n",
    "                attention_mechanism = tf.contrib.seq2seq.BahdanauAttention(\n",
    "                    self.num_hidden * 2, attention_states, memory_sequence_length=self.X_len, normalize=True)\n",
    "                decoder_cell = tf.contrib.seq2seq.AttentionWrapper(decoder_cell, attention_mechanism,\n",
    "                                                                   attention_layer_size=self.num_hidden * 2)\n",
    "                initial_state = decoder_cell.zero_state(dtype=tf.float32, batch_size=self.batch_size)\n",
    "                initial_state = initial_state.clone(cell_state=self.encoder_state)\n",
    "                helper = tf.contrib.seq2seq.TrainingHelper(self.decoder_emb_inp, self.decoder_len, time_major=True)\n",
    "                decoder = tf.contrib.seq2seq.BasicDecoder(decoder_cell, helper, initial_state)\n",
    "                outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(decoder, output_time_major=True, scope=decoder_scope)\n",
    "                self.decoder_output = outputs.rnn_output\n",
    "                self.logits = tf.transpose(\n",
    "                    self.projection_layer(self.decoder_output), perm=[1, 0, 2])\n",
    "                self.logits_reshape = tf.concat(\n",
    "                    [self.logits, tf.zeros([self.batch_size, summary_max_len - tf.shape(self.logits)[1], self.vocabulary_size])], axis=1)\n",
    "            else:\n",
    "                tiled_encoder_output = tf.contrib.seq2seq.tile_batch(\n",
    "                    tf.transpose(self.encoder_output, perm=[1, 0, 2]), multiplier=self.beam_width)\n",
    "                tiled_encoder_final_state = tf.contrib.seq2seq.tile_batch(self.encoder_state, multiplier=self.beam_width)\n",
    "                tiled_seq_len = tf.contrib.seq2seq.tile_batch(self.X_len, multiplier=self.beam_width)\n",
    "                attention_mechanism = tf.contrib.seq2seq.BahdanauAttention(\n",
    "                    self.num_hidden * 2, tiled_encoder_output, memory_sequence_length=tiled_seq_len, normalize=True)\n",
    "                decoder_cell = tf.contrib.seq2seq.AttentionWrapper(decoder_cell, attention_mechanism,\n",
    "                                                                   attention_layer_size=self.num_hidden * 2)\n",
    "                initial_state = decoder_cell.zero_state(dtype=tf.float32, batch_size=self.batch_size * self.beam_width)\n",
    "                initial_state = initial_state.clone(cell_state=tiled_encoder_final_state)\n",
    "                decoder = tf.contrib.seq2seq.BeamSearchDecoder(\n",
    "                    cell=decoder_cell,\n",
    "                    embedding=self.embeddings,\n",
    "                    start_tokens=tf.fill([self.batch_size], tf.constant(2)),\n",
    "                    end_token=tf.constant(3),\n",
    "                    initial_state=initial_state,\n",
    "                    beam_width=self.beam_width,\n",
    "                    output_layer=self.projection_layer\n",
    "                )\n",
    "                outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "                    decoder, output_time_major=True, maximum_iterations=summary_max_len, scope=decoder_scope)\n",
    "                self.prediction = tf.transpose(outputs.predicted_ids, perm=[1, 2, 0])\n",
    "\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            if not forward_only:\n",
    "                crossent = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                    logits=self.logits_reshape, labels=self.decoder_target)\n",
    "                weights = tf.sequence_mask(self.decoder_len, summary_max_len, dtype=tf.float32)\n",
    "                self.loss = tf.reduce_sum(crossent * weights / tf.to_float(self.batch_size))\n",
    "\n",
    "                params = tf.trainable_variables()\n",
    "                gradients = tf.gradients(self.loss, params)\n",
    "                clipped_gradients, _ = tf.clip_by_global_norm(gradients, 5.0)\n",
    "                optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
    "                self.update = optimizer.apply_gradients(zip(clipped_gradients, params), global_step=self.global_step)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kUxYn3lkXhjk"
   },
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 442
    },
    "id": "wYdESWI7Xhjo",
    "outputId": "af95a72b-b00c-46f2-d83f-b7f19654cf5c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building dictionary...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-70edbd7a2d73>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Building dictionary...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m \u001b[0mword_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreversed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marticle_max_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummary_max_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loading training dataset...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marticle_max_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummary_max_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/content/drive/MyDrive/M2/DeepLearning/deeplearningproject/utils.py\u001b[0m in \u001b[0;36mbuild_dict\u001b[0;34m(step, toy)\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_article_list\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtrain_title_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m                 \u001b[0mwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    127\u001b[0m     \"\"\"\n\u001b[1;32m    128\u001b[0m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m     return [token for sent in sentences\n\u001b[0m\u001b[1;32m    130\u001b[0m             for token in _treebank_word_tokenizer.tokenize(sent)]\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m     return [token for sent in sentences\n\u001b[0;32m--> 130\u001b[0;31m             for token in _treebank_word_tokenizer.tokenize(sent)]\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/tokenize/treebank.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text, convert_parentheses, return_str)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mregexp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubstitution\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPUNCTUATION\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubstitution\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;31m# Handles parentheses.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.perf_counter()\n",
    "import tensorflow as tf\n",
    "import argparse\n",
    "import pickle\n",
    "import os\n",
    "from model import Model\n",
    "from utils import build_dict, build_dataset, batch_iter\n",
    "\n",
    "params = {\n",
    "    \"batch_size\":64, \n",
    "    \"beam_width\":10, \n",
    "    \"embedding_size\":300, \n",
    "    \"glove\":False, \n",
    "    \"keep_prob\":0.8, \n",
    "    \"learning_rate\":0.001, \n",
    "    \"num_epochs\":10, \n",
    "    \"num_hidden\":150, \n",
    "    \"num_layers\":2,\n",
    "    \"toy\":False, \n",
    "    \"with_model\":True\n",
    "}\n",
    "class Arg(dict):\n",
    "    \"\"\"dot.notation access to dictionary attributes\"\"\"\n",
    "    __getattr__ = dict.get\n",
    "    __setattr__ = dict.__setitem__\n",
    "    __delattr__ = dict.__delitem__\n",
    "args = Arg(params)\n",
    "\n",
    "# with open(\"args.pickle\", \"wb\") as f:\n",
    "#     pickle.dump(args, f)\n",
    "\n",
    "if not os.path.exists(\"saved_model\"):\n",
    "    os.mkdir(\"saved_model\")\n",
    "else:\n",
    "    if args['with_model'] and not os.path.exists('saved_model/checkpoint'):\n",
    "        old_model_checkpoint_path = open('saved_model/checkpoint', 'r')\n",
    "        old_model_checkpoint_path = \"\".join([\"saved_model/\",old_model_checkpoint_path.read().splitlines()[0].split('\"')[1] ])\n",
    "\n",
    "\n",
    "print(\"Building dictionary...\")\n",
    "word_dict, reversed_dict, article_max_len, summary_max_len = build_dict(\"train\", args.toy)\n",
    "print(\"Loading training dataset...\")\n",
    "train_x, train_y = build_dataset(\"train\", word_dict, article_max_len, summary_max_len, args.toy)\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    model = Model(reversed_dict, article_max_len, summary_max_len, args)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver = tf.train.Saver(tf.global_variables())\n",
    "    if 'old_model_checkpoint_path' in globals():\n",
    "        print(\"Continuing from previous trained model:\" , old_model_checkpoint_path , \"...\")\n",
    "        saver.restore(sess, old_model_checkpoint_path )\n",
    "\n",
    "    batches = batch_iter(train_x, train_y, args.batch_size, args.num_epochs)\n",
    "    num_batches_per_epoch = (len(train_x) - 1) // args.batch_size + 1\n",
    "\n",
    "    print(\"\\nIteration starts.\")\n",
    "    print(\"Number of batches per epoch :\", num_batches_per_epoch)\n",
    "    for batch_x, batch_y in batches:\n",
    "        batch_x_len = list(map(lambda x: len([y for y in x if y != 0]), batch_x))\n",
    "        batch_decoder_input = list(map(lambda x: [word_dict[\"<s>\"]] + list(x), batch_y))\n",
    "        batch_decoder_len = list(map(lambda x: len([y for y in x if y != 0]), batch_decoder_input))\n",
    "        batch_decoder_output = list(map(lambda x: list(x) + [word_dict[\"</s>\"]], batch_y))\n",
    "\n",
    "        batch_decoder_input = list(\n",
    "            map(lambda d: d + (summary_max_len - len(d)) * [word_dict[\"<padding>\"]], batch_decoder_input))\n",
    "        batch_decoder_output = list(\n",
    "            map(lambda d: d + (summary_max_len - len(d)) * [word_dict[\"<padding>\"]], batch_decoder_output))\n",
    "\n",
    "        train_feed_dict = {\n",
    "            model.batch_size: len(batch_x),\n",
    "            model.X: batch_x,\n",
    "            model.X_len: batch_x_len,\n",
    "            model.decoder_input: batch_decoder_input,\n",
    "            model.decoder_len: batch_decoder_len,\n",
    "            model.decoder_target: batch_decoder_output\n",
    "        }\n",
    "\n",
    "        _, step, loss = sess.run([model.update, model.global_step, model.loss], feed_dict=train_feed_dict)\n",
    "\n",
    "        if step % 1000 == 0:\n",
    "            print(\"step {0}: loss = {1}\".format(step, loss))\n",
    "\n",
    "        if step % num_batches_per_epoch == 0:\n",
    "            hours, rem = divmod(time.perf_counter() - start, 3600)\n",
    "            minutes, seconds = divmod(rem, 60)\n",
    "            saver.save(sess, \"./saved_model/model.ckpt\", global_step=step)\n",
    "            print(\" Epoch {0}: Model is saved.\".format(step // num_batches_per_epoch),\n",
    "            \"Elapsed: {:0>2}:{:0>2}:{:05.2f}\".format(int(hours),int(minutes),seconds) , \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jKcQBM8PXhjq"
   },
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7fGxU31MXhjr",
    "outputId": "a7a26793-5686-4857-a961-5068bc5520f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dictionary...\n",
      "Loading validation dataset...\n",
      "Loading saved model...\n",
      "WARNING:tensorflow:From <ipython-input-8-bc36af2aeb5f>:35: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/contrib/rnn/python/ops/rnn.py:239: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/rnn_cell_impl.py:735: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.add_weight` method instead.\n",
      "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/rnn_cell_impl.py:739: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/rnn.py:244: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/contrib/seq2seq/python/ops/beam_search_decoder.py:971: to_int64 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "INFO:tensorflow:Restoring parameters from ./saved_model/model.ckpt-297190\n",
      "Writing summaries to 'result.txt'...\n",
      "Summaries are saved to \"result.txt\"...\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pickle\n",
    "from utils import build_dict, build_dataset, batch_iter\n",
    "\n",
    "\n",
    "with open(\"args.pickle\", \"rb\") as f:\n",
    "    args = pickle.load(f)\n",
    "\n",
    "print(\"Loading dictionary...\")\n",
    "word_dict, reversed_dict, article_max_len, summary_max_len = build_dict(\"valid\", args.toy)\n",
    "print(\"Loading validation dataset...\")\n",
    "valid_x = build_dataset(\"valid\", word_dict, article_max_len, summary_max_len, args.toy)\n",
    "valid_x_len = [len([y for y in x if y != 0]) for x in valid_x]\n",
    "len(valid_x)\n",
    "with tf.Session() as sess:\n",
    "    print(\"Loading saved model...\")\n",
    "    model = Model(reversed_dict, article_max_len, summary_max_len, args, forward_only=True)\n",
    "    saver = tf.train.Saver(tf.global_variables())\n",
    "    ckpt = tf.train.get_checkpoint_state(\"./saved_model/\")\n",
    "    saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "\n",
    "    batches = batch_iter(valid_x, [0] * len(valid_x), args.batch_size, 1)\n",
    "\n",
    "    print(\"Writing summaries to 'result.txt'...\")\n",
    "    for batch_x, _ in batches:\n",
    "        batch_x_len = [len([y for y in x if y != 0]) for x in batch_x]\n",
    "\n",
    "        valid_feed_dict = {\n",
    "            model.batch_size: len(batch_x),\n",
    "            model.X: batch_x,\n",
    "            model.X_len: batch_x_len,\n",
    "        }\n",
    "\n",
    "        prediction = sess.run(model.prediction, feed_dict=valid_feed_dict)\n",
    "        prediction_output = [[reversed_dict[y] for y in x] for x in prediction[:, 0, :]]\n",
    "        summaries = []\n",
    "        with open(\"result.txt\", \"a\") as f:\n",
    "            for line in prediction_output:\n",
    "                summary = list()\n",
    "                for word in line:\n",
    "                    if word == \"</s>\":\n",
    "                        break\n",
    "                    if word not in summary:\n",
    "                        summary.append(word)\n",
    "                s = \" \".join(summary)\n",
    "                summaries.append(s)\n",
    "                print(s, file=f)\n",
    "\n",
    "    print('Summaries are saved to \"result.txt\"...')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CPvl6GTP4Z1E"
   },
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "RlR0lFM-lXsK"
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "SFhsibBJjHeW"
   },
   "outputs": [],
   "source": [
    "summaries_hat = []  \r\n",
    "with open(\"result.txt\", \"r\") as f:\r\n",
    "    for line in f:\r\n",
    "        summaries_hat.append(line[:-1])\r\n",
    "\r\n",
    "summaries = []  \r\n",
    "with open(\"reduceddata/sumdata/train/valid.title.filter.txt\", \"r\") as f:\r\n",
    "    for line in f:\r\n",
    "        summaries.append(line)\r\n",
    "articles = []  \r\n",
    "with open(\"reduceddata/sumdata/train/valid.article.filter.txt\", \"r\") as f:\r\n",
    "    for line in f:\r\n",
    "        articles.append(line)\r\n",
    "\r\n",
    "\r\n",
    "def printRandomExamples(articles, y, y_hat, n_examples = 3):\r\n",
    "    for i in range(n_examples):\r\n",
    "        j = np.random.randint(0, len(articles))\r\n",
    "        print(f'Article {i+1} -----------------')\r\n",
    "        print(articles[j])\r\n",
    "        print('Original summary -----------------')\r\n",
    "        print(y[j])\r\n",
    "        print('Generated summary -----------------')\r\n",
    "        print(y_hat[j])\r\n",
    "        print('\\n')\r\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "80uf-BSGjYc7",
    "outputId": "775ca444-ec21-40ec-e208-482f5349654f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article 1 -----------------\n",
      "demand for condoms in conservative muslim-majority indian kashmir has outstripped supply and the state authorities have asked for more , an official said sunday .\n",
      "\n",
      "Original summary -----------------\n",
      "indian kashmir seeks more condoms\n",
      "\n",
      "Generated summary -----------------\n",
      "demand for condoms in conservative indian kashmir\n",
      "\n",
      "\n",
      "Article 2 -----------------\n",
      "england under-## centre-back anton ferdinand , younger brother of manchester united 's rio , has signed a contract extension with west ham that will keep him at upton park until #### .\n",
      "\n",
      "Original summary -----------------\n",
      "ferdinand hammers out new contract\n",
      "\n",
      "Generated summary -----------------\n",
      "ferdinand extends contract with west ham\n",
      "\n",
      "\n",
      "Article 3 -----------------\n",
      "germany 's patric leitner and alexander resch won the sixth leg of the eight-race doubles luge world cup here friday , to move to the top of the overall standings .\n",
      "\n",
      "Original summary -----------------\n",
      "afp world news summary\n",
      "\n",
      "Generated summary -----------------\n",
      "nephew win luge world cup\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "printRandomExamples(articles, summaries, summaries_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ux9xycxu36DF"
   },
   "source": [
    "https://pypi.org/project/py-rouge/\r\n",
    "* The folder \"rouge\" of this package should be placed in the root of the project directory\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "rhHexz_FvTzq"
   },
   "outputs": [],
   "source": [
    "import rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "_683cKDeywmA"
   },
   "outputs": [],
   "source": [
    "evaluator = rouge.Rouge(metrics=['rouge-n', 'rouge-l'],\r\n",
    "                        max_n = 2,\r\n",
    "                        limit_length=True,\r\n",
    "                        length_limit=15,\r\n",
    "                        length_limit_type='words',\r\n",
    "                        apply_avg=True,\r\n",
    "                        stemming=False)\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "LPii8oa40Mwi"
   },
   "outputs": [],
   "source": [
    "scores = evaluator.get_scores(summaries_hat, summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "X0xoDoPS2GBM"
   },
   "outputs": [],
   "source": [
    "def prepare_results(p, r, f):\r\n",
    "    return '\\t{}:\\t{}: {:5.2f}\\t{}: {:5.2f}\\t{}: {:5.2f}'.format(metric, 'P', 100.0 * p, 'R', 100.0 * r, 'F1', 100.0 * f)\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j795OFLG1hoU",
    "outputId": "ab069da5-0f1f-4150-d706-10152d6f17e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\trouge-1:\tP: 48.96\tR: 39.89\tF1: 43.18\n",
      "\trouge-2:\tP: 23.74\tR: 19.16\tF1: 20.76\n",
      "\trouge-l:\tP: 46.57\tR: 38.00\tF1: 41.11\n"
     ]
    }
   ],
   "source": [
    "for metric, results in sorted(scores.items(), key=lambda x: x[0]):\r\n",
    "     print(prepare_results(results['p'], results['r'], results['f']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rjKSgBoY7LkN"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of all_in_one.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
